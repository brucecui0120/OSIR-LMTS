This project aims to establish a systematic framework for evaluating the impact of open-source large model technology systems. By defining scientific and actionable evaluation criteria, we strive to help developers, researchers, and enterprises better understand and assess the practical value of these systems. Our approach focuses on four key technical dimensions: data, models, systems, and benchmarks.

### **Evaluation Dimensions**

The evaluation framework includes the following dimensions:

1. **Data Dimension**: Assesses the diversity, quality, and accessibility of data resources to ensure models demonstrate generalization and adaptability across diverse tasks.
2. **Model Dimension**: Evaluates model innovation, including algorithm architecture, training strategies, and multimodal adaptability, to measure the depth of innovation and potential for practical application.
3. **System Dimension**: Examines hardware and technical support capabilities, assessing the performance, compatibility, and ease of deployment of open-source projects across various hardware platforms.
4. **Benchmark Dimension**: Focuses on the effectiveness and stability of models in real-world tasks and scenarios, reflecting their reliability and applicability.

This holistic coverage ensures a systematic and rigorous evaluation of the influence of open-source large model technology systems.

### **Data Collection Channels**

Key channels for data collection include:

1. **Platforms**: Huggingface, ModelScope, GitHub, PyTorch Community.
2. **Datasets**: Google Dataset Search, Baidu Wenyan, BAAI DataHub, Nvidia NGC, and other official platforms.

### **Statistical Methods**

- **Data Dimension**: Monthly download counts for all relevant datasets.
- **Model Dimension**: Monthly download counts for the top 100 models per organization.
- **System Dimension**: Cumulative values for all metrics as of the reporting date.
- **Benchmark Dimension**: Cumulative values for all metrics as of the reporting date.

### **Engagement and Feedback**

We encourage active participation from the community. You can share your suggestions and feedback by scanning the QR code below or submitting an issue directly. Your input will help us refine the evaluation methodology and enhance data accuracy and completeness.

<div align=center>
<img src="./contract_logo.jpg" width="30%" height="30%">
</div>

Thank you for your attention and support for the Open Source Large Model Technology Influence Leaderboard. Together, we can drive innovation and advancement in open-source technologies!