This project aims to establish a systematic framework for evaluating the impact of open-source large model technology systems. By defining scientific and actionable evaluation criteria, we strive to help developers, researchers, and enterprises better understand and assess the practical value of these systems. Our approach focuses on four key technical dimensions: data, models, systems, and benchmarks.

## **Evaluation Dimensions**

The evaluation framework includes the following dimensions:

- **Data Dimension**: Includes dataset coverage, lifecycle coverage of large models, industry-specific data, and data processing tools.
- **Model Dimension**: Encompasses usage metrics, modality coverage, model scale, contributor activity, and openness of the modelâ€™s source code.
- **System Dimension**: Covers operator libraries, compatibility testing, parallel training and inference frameworks, communication libraries, deep learning frameworks, and contributor activity.
- **Benchmark Dimension**: Focuses on benchmark leaderboards, evaluated models, datasets, and evaluation methodologies.

This holistic coverage ensures a systematic and rigorous evaluation of the influence of open-source large model technology systems.

## **Data Sources and Evaluation Methods**

### **Data Sources:**

Key channels for data collection include:

- **Data Dimension**: GitHub, Hugging Face, ModelScope, OpenI, OpenDataLab, BAAI official website, Google official website, META official website, and select other platforms.
- **Model Dimension**: GitHub, Hugging Face, ModelScope, OpenI, etc.
- **System Dimension**: GitHub repositories for DIOPI, BladeDISC, PyTorch, Paddle, MindSpore, TensorFlow, etc.
- **Benchmark Dimension**: Hugging Face, GitHub, Gitee, and official websites of various organizations, etc.

### **Statistical Methods**

- **Data Dimension**: GitHub, Hugging Face, ModelScope, OpenI, OpenDataLab, BAAI official website, Google official website, META official website, and select other platforms.
- **Model Dimension**: GitHub, Hugging Face, ModelScope, OpenI, etc.
- **System Dimension**: GitHub repositories for DIOPI, BladeDISC, PyTorch, Paddle, MindSpore, TensorFlow, etc.
- **Benchmark Dimension**: Hugging Face, GitHub, Gitee, and official websites of various organizations, etc.

## Calculation Method

All metrics are normalized using the Min-Max method, and the influence score is calculated as the average of the normalized values.

## **Engagement and Feedback**

We encourage active participation from the community. You can share your suggestions and feedback by scanning the QR code below or submitting an issue directly. Your input will help us refine the evaluation methodology and enhance data accuracy and completeness.

<div align=center>
<img src="./contract_logo.jpg" width="30%" height="30%">
</div>

Thank you for your attention and support for the Open Source Large Model Technology Influence Leaderboard. Together, we can drive innovation and advancement in open-source technologies!